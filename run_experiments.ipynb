{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.rl.cql_dqn import *\n",
    "from src.rl.rec_replay_buffer import RecReplayBuffer\n",
    "from RECE.data import get_dataset, data_to_sequences, SequentialDataset\n",
    "from RECE.train import prepare_sasrec_model, train_sasrec_epoch, downvote_seen_items, sasrec_model_scoring, topn_recommendations, model_evaluate\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from clearml import Task, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sasrec_config = dict(\n",
    "    manual_seed = 123,\n",
    "    sampler_seed = 123,\n",
    "    num_epochs = 100, #3 10 22 100&dropout0.9&hd32&bs1000\n",
    "    maxlen = 100,\n",
    "    hidden_units = 64,\n",
    "    dropout_rate = 0.3,\n",
    "    num_blocks = 2,\n",
    "    num_heads = 1,\n",
    "    batch_size = 128, #DEBUG\n",
    "    learning_rate = 1e-3,\n",
    "    fwd_type = 'ce',\n",
    "    l2_emb = 0,\n",
    "    patience = 10,\n",
    "    skip_epochs = 1,\n",
    "    n_neg_samples=0,\n",
    "    sampling='no_sampling'\n",
    ")\n",
    "\n",
    "\n",
    "config = TrainConfig(\n",
    "    orthogonal_init = True,\n",
    "    q_n_hidden_layers = 1,\n",
    "    qf_lr = 3e-4,\n",
    "    batch_size=sasrec_config['batch_size'],\n",
    "    device=\"cuda\",\n",
    "    bc_steps=100000,\n",
    "    cql_alpha=100.0,\n",
    "\n",
    "    env=\"MovieLens\",\n",
    "    project= \"CQL-SASREC\",\n",
    "    group= \"CQL-SASREC\",\n",
    "    name= \"CQL\",\n",
    "    #cql_negative_samples = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:eb3926d8-e92c-47f3-8503-97be593d9b28) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CQL-halfcheetah-medium-expert-v2-85495f29</strong> at: <a href='https://wandb.ai/razvors_team/CQL-SASREC/runs/eb3926d8-e92c-47f3-8503-97be593d9b28' target=\"_blank\">https://wandb.ai/razvors_team/CQL-SASREC/runs/eb3926d8-e92c-47f3-8503-97be593d9b28</a><br/> View project at: <a href='https://wandb.ai/razvors_team/CQL-SASREC' target=\"_blank\">https://wandb.ai/razvors_team/CQL-SASREC</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241017_163902-eb3926d8-e92c-47f3-8503-97be593d9b28\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:eb3926d8-e92c-47f3-8503-97be593d9b28). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\razvor\\Documents\\DS\\Skoltech\\Research\\RL\\corl-sasrec\\wandb\\run-20241017_164104-f6974723-9092-4cd5-b184-03db0be6e682</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/razvors_team/CQL-SASREC/runs/f6974723-9092-4cd5-b184-03db0be6e682' target=\"_blank\">CQL-MovieLens-62ab256d</a></strong> to <a href='https://wandb.ai/razvors_team/CQL-SASREC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/razvors_team/CQL-SASREC' target=\"_blank\">https://wandb.ai/razvors_team/CQL-SASREC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/razvors_team/CQL-SASREC/runs/f6974723-9092-4cd5-b184-03db0be6e682' target=\"_blank\">https://wandb.ai/razvors_team/CQL-SASREC/runs/f6974723-9092-4cd5-b184-03db0be6e682</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"d819ea0d92a856b5544d1aa919f503250223447c\" # Change to your W&B profile if you need it\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "\n",
    "seed = config.seed\n",
    "set_seed(seed)\n",
    "wandb_init(asdict(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 115 invalid observations.\n",
      "Filtered 11 invalid observations.\n",
      "Filtered 4 invalid observations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'users': 'userid',\n",
       " 'items': 'itemid',\n",
       " 'order': 'timestamp',\n",
       " 'n_users': 5400,\n",
       " 'n_items': 3658}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_temp, data_description_temp, testset_valid_temp, testset, holdout_valid_temp, _ = get_dataset(splitting='temporal_full',\n",
    "                                                                                     q=0.8)\n",
    "data_description_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sasrec_model, sampler, n_batches, optimizers = prepare_sasrec_model(sasrec_config, training_temp, data_description_temp, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = log = None\n",
    "\n",
    "def pretrain(model, config, data_description, testset_valid, holdout_valid):   \n",
    "    losses = {}\n",
    "    metrics = {}\n",
    "    ndcg = {}\n",
    "    best_ndcg = 0\n",
    "    wait = 0\n",
    "\n",
    "    start_time = time()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "    checkpt_name = uuid.uuid4().hex\n",
    "    if not os.path.exists('./checkpt'):\n",
    "        os.mkdir('./checkpt')\n",
    "    \n",
    "    checkpt_path = os.path.join('./checkpt', f'{checkpt_name}.chkpt')\n",
    "\n",
    "    for epoch in (range(config['num_epochs'])):\n",
    "        losses[epoch] = train_sasrec_epoch(\n",
    "            model, n_batches, config['l2_emb'], sampler, optimizers, device\n",
    "        )\n",
    "        if epoch % config['skip_epochs'] == 0:\n",
    "            val_scores = sasrec_model_scoring(model, testset_valid, data_description, device)\n",
    "            downvote_seen_items(val_scores, testset_valid, data_description)\n",
    "            val_recs = topn_recommendations(val_scores, topn=10)\n",
    "            val_metrics = model_evaluate(val_recs, holdout_valid, data_description)\n",
    "            metrics[epoch] = val_metrics\n",
    "            ndcg_ = val_metrics['ndcg@10']\n",
    "            ndcg[epoch] = ndcg_\n",
    "\n",
    "            print(f'Epoch {epoch}, NDCG@10: {ndcg_}')\n",
    "            \n",
    "            if task and (epoch % 5 == 0):\n",
    "                log.report_scalar(\"Loss\", series='Val', iteration=epoch, value=np.mean(losses[epoch]))\n",
    "                log.report_scalar(\"NDCG\", series='Val', iteration=epoch, value=ndcg_)\n",
    "\n",
    "            if ndcg_ > best_ndcg:\n",
    "                best_ndcg = ndcg_\n",
    "                torch.save(model.state_dict(), checkpt_path)\n",
    "                wait = 0\n",
    "            elif wait < config['patience'] // config['skip_epochs'] + 1:\n",
    "                wait += 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    training_time_sec = time() - start_time\n",
    "    full_peak_training_memory_bytes = torch.cuda.max_memory_allocated()\n",
    "    peak_training_memory_bytes = torch.cuda.max_memory_allocated() - start_memory\n",
    "    training_epoches = len(losses)\n",
    "    \n",
    "    model.load_state_dict(torch.load(checkpt_path))\n",
    "    os.remove(checkpt_path)\n",
    "\n",
    "    print()\n",
    "    print('Peak training memory, mb:', round(full_peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "    print('Training epoches:', training_epoches)\n",
    "    print('Training time, m:', round(training_time_sec/ 60., 2))\n",
    "    \n",
    "    if task:\n",
    "        ind_max = np.argmax(list(ndcg.values())) * config['skip_epochs']\n",
    "        for metric_name, metric_value in metrics[ind_max].items():\n",
    "            log.report_single_value(name=f'val_{metric_name}', value=round(metric_value, 4))\n",
    "        log.report_single_value(name='train_peak_mem_mb', value=round(peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "        log.report_single_value(name='full_train_peak_mem_mb', value=round(full_peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "        log.report_single_value(name='train_epoches', value=training_epoches)\n",
    "        log.report_single_value(name='train_time_m', value=round(training_time_sec/ 60., 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, NDCG@10: 0.020602853670914675\n",
      "Epoch 1, NDCG@10: 0.022537926101137903\n",
      "Epoch 2, NDCG@10: 0.03079701581977307\n",
      "Epoch 3, NDCG@10: 0.05572657924072975\n",
      "Epoch 4, NDCG@10: 0.0759097182715343\n",
      "Epoch 5, NDCG@10: 0.0909473590097159\n",
      "Epoch 6, NDCG@10: 0.10210885287687085\n",
      "Epoch 7, NDCG@10: 0.10908023379450235\n",
      "Epoch 8, NDCG@10: 0.1166471985471137\n",
      "Epoch 9, NDCG@10: 0.12192767949789424\n",
      "Epoch 10, NDCG@10: 0.12455379862998373\n",
      "Epoch 11, NDCG@10: 0.1296894701894721\n",
      "Epoch 12, NDCG@10: 0.13408232137587925\n",
      "Epoch 13, NDCG@10: 0.13641051296376924\n",
      "Epoch 14, NDCG@10: 0.13819931679851163\n",
      "Epoch 15, NDCG@10: 0.13939482776629394\n",
      "Epoch 16, NDCG@10: 0.14317104566636943\n",
      "Epoch 17, NDCG@10: 0.14352014098851246\n",
      "Epoch 18, NDCG@10: 0.1451760569770132\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msasrec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msasrec_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_description_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset_valid_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholdout_valid_temp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[153], line 26\u001b[0m, in \u001b[0;36mpretrain\u001b[1;34m(model, config, data_description, testset_valid, holdout_valid)\u001b[0m\n\u001b[0;32m     22\u001b[0m losses[epoch] \u001b[38;5;241m=\u001b[39m train_sasrec_epoch(\n\u001b[0;32m     23\u001b[0m     model, n_batches, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_emb\u001b[39m\u001b[38;5;124m'\u001b[39m], sampler, optimizers, device\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     val_scores \u001b[38;5;241m=\u001b[39m \u001b[43msasrec_model_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     downvote_seen_items(val_scores, testset_valid, data_description)\n\u001b[0;32m     28\u001b[0m     val_recs \u001b[38;5;241m=\u001b[39m topn_recommendations(val_scores, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\razvor\\Documents\\DS\\Skoltech\\Research\\RL\\corl-sasrec\\RECE\\eval_utils.py:25\u001b[0m, in \u001b[0;36msasrec_model_scoring\u001b[1;34m(params, data, data_description, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, seq \u001b[38;5;129;01min\u001b[39;00m test_sequences\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 25\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(predictions\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\razvor\\Documents\\DS\\Skoltech\\Research\\RL\\corl-sasrec\\RECE\\model.py:126\u001b[0m, in \u001b[0;36mSASRecBackBone.score\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    124\u001b[0m log_seqs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull([maxlen], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mseq\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    125\u001b[0m log_seqs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(seq):] \u001b[38;5;241m=\u001b[39m seq[\u001b[38;5;241m-\u001b[39mmaxlen:]\n\u001b[1;32m--> 126\u001b[0m log_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog2feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_seqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m final_feat \u001b[38;5;241m=\u001b[39m log_feats[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# only use last QKV classifier\u001b[39;00m\n\u001b[0;32m    129\u001b[0m item_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_emb\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[1;32mc:\\Users\\razvor\\Documents\\DS\\Skoltech\\Research\\RL\\corl-sasrec\\RECE\\model.py:89\u001b[0m, in \u001b[0;36mSASRecBackBone.log2feats\u001b[1;34m(self, log_seqs)\u001b[0m\n\u001b[0;32m     87\u001b[0m seqs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(seqs, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     88\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layernorms[i](seqs)\n\u001b[1;32m---> 89\u001b[0m mha_outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m seqs \u001b[38;5;241m=\u001b[39m Q \u001b[38;5;241m+\u001b[39m mha_outputs\n\u001b[0;32m     94\u001b[0m seqs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(seqs, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\razvor\\Documents\\DS\\Skoltech\\Research\\RL\\corl-sasrec\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\razvor\\Documents\\DS\\Skoltech\\Research\\RL\\corl-sasrec\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\razvor\\Documents\\DS\\Skoltech\\Research\\RL\\corl-sasrec\\.conda\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\razvor\\Documents\\DS\\Skoltech\\Research\\RL\\corl-sasrec\\.conda\\lib\\site-packages\\torch\\nn\\functional.py:5523\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5520\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_causal \u001b[38;5;129;01mand\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFIXME: is_causal not implemented for need_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 5523\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaddbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5525\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain(sasrec_model, sasrec_config, data_description_temp, testset_valid_temp, holdout_valid_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sasrec_model.fwd_type = 'embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_dim = data_description_temp['n_items']+2\n",
    "action_dim = data_description_temp['n_items']+2\n",
    "\n",
    "replay_buffer = RecReplayBuffer(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    config.buffer_size,\n",
    "    config.device,\n",
    "    sampler\n",
    ")\n",
    "\n",
    "max_action = float(1)\n",
    "\n",
    "if config.checkpoints_path is not None:\n",
    "    print(f\"Checkpoints path: {config.checkpoints_path}\")\n",
    "    os.makedirs(config.checkpoints_path, exist_ok=True)\n",
    "    with open(os.path.join(config.checkpoints_path, \"config.yaml\"), \"w\") as f:\n",
    "        pyrallis.dump(config, f)\n",
    "\n",
    "# Set seeds\n",
    "seed = config.seed\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "q_1 = FullyConnectedQFunction(\n",
    "    64,\n",
    "    action_dim,\n",
    "    config.orthogonal_init,\n",
    "    config.q_n_hidden_layers\n",
    ").to(config.device)\n",
    "\n",
    "q_2 = FullyConnectedQFunction(64, action_dim, config.orthogonal_init, config.q_n_hidden_layers).to(\n",
    "    config.device\n",
    ")\n",
    "q_1_optimizer = torch.optim.Adam(list(q_1.parameters()), config.qf_lr)\n",
    "q_2_optimizer = torch.optim.Adam(list(q_2.parameters()), config.qf_lr)\n",
    "\n",
    "kwargs = {\n",
    "    \"body\": sasrec_model,\n",
    "    \"body_optimizer\": optimizers,\n",
    "    \"q_1\": q_1,\n",
    "    \"q_2\": q_2,\n",
    "    \"q_1_optimizer\": q_1_optimizer,\n",
    "    \"q_2_optimizer\": q_2_optimizer,\n",
    "    \"discount\": config.discount,\n",
    "    \"soft_target_update_rate\": config.soft_target_update_rate,\n",
    "    \"device\": config.device,\n",
    "    # CQL\n",
    "    \"target_entropy\": 1,\n",
    "    \"alpha_multiplier\": config.alpha_multiplier,\n",
    "    \"use_automatic_entropy_tuning\": config.use_automatic_entropy_tuning,\n",
    "    \"backup_entropy\": config.backup_entropy,\n",
    "    \"policy_lr\": config.policy_lr,\n",
    "    \"qf_lr\": config.qf_lr,\n",
    "    \"bc_steps\": config.bc_steps,\n",
    "    \"target_update_period\": config.target_update_period,\n",
    "    \"cql_n_actions\": config.cql_n_actions,\n",
    "    \"cql_importance_sample\": config.cql_importance_sample,\n",
    "    \"cql_lagrange\": config.cql_lagrange,\n",
    "    \"cql_target_action_gap\": config.cql_target_action_gap,\n",
    "    \"cql_temp\": config.cql_temp,\n",
    "    \"cql_alpha\": config.cql_alpha,\n",
    "    \"cql_max_target_backup\": config.cql_max_target_backup,\n",
    "    \"cql_clip_diff_min\": config.cql_clip_diff_min,\n",
    "    \"cql_clip_diff_max\": config.cql_clip_diff_max,\n",
    "    \"cql_negative_samples\": 10\n",
    "}\n",
    "\n",
    "trainer = DQNCQL(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_epoch():\n",
    "    trainer.q_1.train()\n",
    "    trainer.q_2.train()\n",
    "    trainer.body.train()\n",
    "    losses = []\n",
    "    N = len(sampler)\n",
    "    for t in range(N):\n",
    "        batch = replay_buffer.sample(config.batch_size)\n",
    "        batch = [b.to(config.device) for b in batch]\n",
    "        log_dict = trainer.train(batch)\n",
    "        losses.append(log_dict['loss'])\n",
    "        if t % 100 == 1:\n",
    "            print(f\"Iter {t} of {N}. Train loss: \", np.mean(losses[-100:]))\n",
    "    return np.mean(losses)\n",
    "\n",
    "def agent_model_scoring(data, data_description, device):\n",
    "    trainer.q_1.eval()\n",
    "    trainer.q_2.eval()\n",
    "    trainer.body.eval()\n",
    "    test_sequences = data_to_sequences(data, data_description)\n",
    "    # perform scoring on a user-batch level\n",
    "    scores = []\n",
    "    for _, seq in test_sequences.items():\n",
    "        with torch.no_grad():\n",
    "            body_out = trainer.body.score_with_state(torch.tensor(seq, device=device, dtype=torch.long))[-1]\n",
    "            body_out = body_out.reshape(-1, body_out.shape[-1])\n",
    "            predictions = (q_1(body_out) + q_2(body_out)) / 2.0\n",
    "        scores.append(predictions.detach().cpu().numpy())\n",
    "    return np.concatenate(scores, axis=0)\n",
    "\n",
    "def train_agent(config, data_description, testset_valid, holdout_valid):   \n",
    "    losses = {}\n",
    "    metrics = {}\n",
    "    ndcg = {}\n",
    "    best_ndcg = 0\n",
    "    wait = 0\n",
    "\n",
    "    start_time = time()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "    checkpt_name = uuid.uuid4().hex\n",
    "    if not os.path.exists('./checkpt'):\n",
    "        os.mkdir('./checkpt')\n",
    "    \n",
    "    checkpt_path = os.path.join('./checkpt', f'{checkpt_name}.chkpt')\n",
    "\n",
    "    for epoch in (range(config['num_epochs'])):\n",
    "        losses[epoch] = train_agent_epoch()\n",
    "        wandb.log({\n",
    "            \"train_loss\": losses[epoch]\n",
    "        }, step=trainer.total_it)\n",
    "        if epoch % config['skip_epochs'] == 0:\n",
    "            val_scores = agent_model_scoring(testset_valid, data_description, device)\n",
    "            downvote_seen_items(val_scores, testset_valid, data_description)\n",
    "            val_recs = topn_recommendations(val_scores, topn=10)\n",
    "            val_metrics = model_evaluate(val_recs, holdout_valid, data_description)\n",
    "            metrics[epoch] = val_metrics\n",
    "            ndcg_ = val_metrics['ndcg@10']\n",
    "            ndcg[epoch] = ndcg_\n",
    "\n",
    "            print(f'Epoch {epoch}, NDCG@10: {ndcg_}')\n",
    "            wandb.log({\n",
    "                \"valid NDCG@10\": ndcg_\n",
    "            }, step=trainer.total_it)\n",
    "            \n",
    "            if task and (epoch % 5 == 0):\n",
    "                log.report_scalar(\"Loss\", series='Val', iteration=epoch, value=np.mean(losses[epoch]))\n",
    "                log.report_scalar(\"NDCG\", series='Val', iteration=epoch, value=ndcg_)\n",
    "\n",
    "            if ndcg_ > best_ndcg:\n",
    "                best_ndcg = ndcg_\n",
    "                #torch.save(model.state_dict(), checkpt_path)\n",
    "                wait = 0\n",
    "            elif wait < config['patience'] // config['skip_epochs'] + 1:\n",
    "                wait += 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    training_time_sec = time() - start_time\n",
    "    full_peak_training_memory_bytes = torch.cuda.max_memory_allocated()\n",
    "    peak_training_memory_bytes = torch.cuda.max_memory_allocated() - start_memory\n",
    "    training_epoches = len(losses)\n",
    "    \n",
    "    #model.load_state_dict(torch.load(checkpt_path))\n",
    "    #trainer.load_state_dict()\n",
    "    #os.remove(checkpt_path)\n",
    "\n",
    "    print()\n",
    "    print('Peak training memory, mb:', round(full_peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "    print('Training epoches:', training_epoches)\n",
    "    print('Training time, m:', round(training_time_sec/ 60., 2))\n",
    "    \n",
    "    if task:\n",
    "        ind_max = np.argmax(list(ndcg.values())) * config['skip_epochs']\n",
    "        for metric_name, metric_value in metrics[ind_max].items():\n",
    "            log.report_single_value(name=f'val_{metric_name}', value=round(metric_value, 4))\n",
    "        log.report_single_value(name='train_peak_mem_mb', value=round(peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "        log.report_single_value(name='full_train_peak_mem_mb', value=round(full_peak_training_memory_bytes/ 1024. / 1024., 2))\n",
    "        log.report_single_value(name='train_epoches', value=training_epoches)\n",
    "        log.report_single_value(name='train_time_m', value=round(training_time_sec/ 60., 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(sasrec_config, data_description_temp, testset_valid_temp, holdout_valid_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 1])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 1.0, lr = 3-e4, pretrained \n",
    "Epoch 3, NDCG@10: 0.09942672790465602\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 100.0, lr = 3-e4, pretrained \n",
    "Epoch 23, NDCG@10: 0.1265350095744121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
